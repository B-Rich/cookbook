<p>A regression model for X on p which writes <span class="math">E(<em>X</em>∣<em>p</em>) = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>p</em></span> is called a linear regression.</p>
<p>Often we would assume that X is normally distributed with constant variance <span class="math"><em>σ</em><sup>2</sup></span> (which is unknown, but independent of p). In this case if we have a sample of n pairs of observations <span class="math">(<em>p</em><sub><em>i</em></sub>, <em>x</em><sub><em>i</em></sub>), </span> we can find the maximum likelihood estimators <span class="math"><em>b</em><sub>1</sub>, <em>b</em><sub>2</sub></span> of <span class="math"><em>β</em><sub>1</sub>, <em>β</em><sub>2</sub></span> by the method of least squares, i.e. we minimize <span class="math">∑<sub><em>i</em></sub>(<em>x</em><sub><em>i</em></sub> − <em>β</em><sub>0</sub> − <em>β</em><sub>1</sub><em>p</em><sub><em>i</em></sub>)<sup>2</sup>.</span></p>
<p>These estimators are: <br /><span class="math">$$b_1 = \frac {\sum _i ( p_i - \bar{p} ) ( x_i - \bar{x} ) }
{ \sum _i ( p_i - \bar{p} ) ^2 } ,$$</span><br /> <br /><span class="math"><em>b</em><sub>0</sub> = <em>x̄</em> − <em>b</em><sub>1</sub><em>p̄</em>, </span><br /> where <span class="math"><em>p̄</em>, <em>x̄</em></span> are the means of the <span class="math"><em>p</em><sub><em>i</em></sub><em>a</em><em>n</em><em>d</em><em>x</em><sub><em>i</em></sub> : </span> <span class="math">$ \bar{p} = \sum _i \frac{p_i}{n} $</span> and <span class="math">$ \bar{x} = \sum _i \frac{x_i}{n} . $</span></p>
